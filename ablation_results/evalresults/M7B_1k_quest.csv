seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,mistralai/Mistral-7B-v0.1,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,mistral,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_mistralai_Mistral-7B-v0.1_False_mistral_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_False_1_False_False_False_False_4_8_2/0.001_32_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_1024_1024_1_32_4_16_4_MSE_False_False_M7B_1k.csv_M7B_1k_True_0.3875000000000002_20250112-104027.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,32,,False,1000,20,1024,fixed_10pc,quest,AllContextual_ICML,2500,4,1024,1024,1,32,4,16,4,MSE,True,False,M7B_1k_quest.csv,M7B_1k_quest,True,0.09687500000000004,9.34098290745169,6.482259273529053,0.7542087542087542,0.47,0.7780195865070729,0.6361483820047356
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,mistralai/Mistral-7B-v0.1,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,mistral,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_mistralai_Mistral-7B-v0.1_False_mistral_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_False_1_False_False_False_False_4_8_2/0.001_32_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_1024_1024_1_32_4_16_4_MSE_False_False_M7B_1k.csv_M7B_1k_True_0.3875000000000002_20250112-104027.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,32,,False,1000,20,1024,fixed_20pc,quest,AllContextual_ICML,2500,4,1024,1024,1,32,4,16,4,MSE,True,False,M7B_1k_quest.csv,M7B_1k_quest,True,0.1937500000000001,18.924318812787533,7.382894515991211,0.6738215488215489,0.426,0.7279651795429815,0.5390686661404893
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,mistralai/Mistral-7B-v0.1,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,mistral,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_mistralai_Mistral-7B-v0.1_False_mistral_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_False_1_False_False_False_False_4_8_2/0.001_32_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_1024_1024_1_32_4_16_4_MSE_False_False_M7B_1k.csv_M7B_1k_True_0.3875000000000002_20250112-104027.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,32,,False,1000,20,1024,fixed_30pc,quest,AllContextual_ICML,2500,4,1024,1024,1,32,4,16,4,MSE,True,False,M7B_1k_quest.csv,M7B_1k_quest,True,0.290625,28.4225070849061,8.924078941345215,0.6191077441077442,0.398,0.6969532100108814,0.5019731649565904
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,mistralai/Mistral-7B-v0.1,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,mistral,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_mistralai_Mistral-7B-v0.1_False_mistral_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_False_1_False_False_False_False_4_8_2/0.001_32_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_1024_1024_1_32_4_16_4_MSE_False_False_M7B_1k.csv_M7B_1k_True_0.3875000000000002_20250112-104027.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,32,,False,1000,20,1024,fixed_40pc,quest,AllContextual_ICML,2500,4,1024,1024,1,32,4,16,4,MSE,True,False,M7B_1k_quest.csv,M7B_1k_quest,True,0.3875000000000002,37.93969852849841,12.383382797241211,0.5589225589225589,0.3804,0.6795429815016322,0.5035516969218626
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,mistralai/Mistral-7B-v0.1,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,mistral,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_mistralai_Mistral-7B-v0.1_False_mistral_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_False_1_False_False_False_False_4_8_2/0.001_32_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_1024_1024_1_32_4_16_4_MSE_False_False_M7B_1k.csv_M7B_1k_True_0.3875000000000002_20250112-104027.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,32,,False,1000,20,1024,fixed_10pc,quest,AllContextual_ICML,2500,4,1024,1024,1,32,4,16,4,MSE,True,False,M7B_1k_quest.csv,M7B_1k_quest,True,0.09687500000000004,9.34098290745169,6.482259273529053,0.7542087542087542,0.47,0.7780195865070729,0.6361483820047356
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,mistralai/Mistral-7B-v0.1,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,mistral,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_mistralai_Mistral-7B-v0.1_False_mistral_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_False_1_False_False_False_False_4_8_2/0.001_32_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_1024_1024_1_32_4_16_4_MSE_False_False_M7B_1k.csv_M7B_1k_True_0.3875000000000002_20250112-104027.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,32,,False,1000,20,1024,fixed_50pc,quest,AllContextual_ICML,2500,4,1024,1024,1,32,4,16,4,MSE,True,False,M7B_1k_quest.csv,M7B_1k_quest,True,0.484375,47.57619285956025,19.73920249938965,0.5370370370370371,0.3504,0.6463547334058759,0.5185477505919495
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,mistralai/Mistral-7B-v0.1,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,mistral,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_mistralai_Mistral-7B-v0.1_False_mistral_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_False_1_False_False_False_False_4_8_2/0.001_32_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_1024_1024_1_32_4_16_4_MSE_False_False_M7B_1k.csv_M7B_1k_True_0.3875000000000002_20250112-104027.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,32,,False,1000,20,1024,fixed_60pc,quest,AllContextual_ICML,2500,4,1024,1024,1,32,4,16,4,MSE,True,False,M7B_1k_quest.csv,M7B_1k_quest,True,0.58125,57.054818980395794,32.844173431396484,0.4877946127946128,0.342,0.6273122959738846,0.5201262825572218
