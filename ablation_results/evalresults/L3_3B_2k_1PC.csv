seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_10pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.09642857142857146,9.312903269061021,9.001436233520508,0.747895622895623,0.4924,0.7687704026115343,0.6890292028413575
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_20pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.19285714285714292,18.848238087126187,9.0248441696167,0.7445286195286195,0.494,0.7682263329706203,0.6858721389108129
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_30pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.2892857142857142,28.339108398982457,9.06296443939209,0.742003367003367,0.4876,0.7665941240478781,0.6795580110497238
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_40pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.38571428571428584,37.83814534544945,9.126907348632812,0.7285353535353535,0.4812,0.7725788900979326,0.6432517758484609
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_50pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.48214285714285715,47.4150935454028,9.225749969482422,0.7154882154882155,0.4712,0.7627856365614799,0.6179952644041041
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_60pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.5785714285714284,56.8004588995661,9.380229949951172,0.6948653198653199,0.4616,0.7453754080522307,0.584846093133386
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_10pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.09642857142857146,9.312903269061021,9.001436233520508,0.747895622895623,0.4924,0.7687704026115343,0.6890292028413575
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_20pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.19285714285714292,18.848238087126187,9.0248441696167,0.7445286195286195,0.494,0.7682263329706203,0.6858721389108129
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_30pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.2892857142857142,28.339108398982457,9.06296443939209,0.742003367003367,0.4876,0.7665941240478781,0.6795580110497238
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_40pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.38571428571428584,37.83814534544945,9.126907348632812,0.7285353535353535,0.4812,0.7725788900979326,0.6432517758484609
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_50pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.48214285714285715,47.4150935454028,9.225749969482422,0.7154882154882155,0.4712,0.7627856365614799,0.6179952644041041
seed,model_path,task_list,offloading,architecture,channel,heavy_const,q_bits,finetune_dataset,head_sparsity_aggression,do_downstream_eval,do_longbench_eval,longbench_datasets,model_mode,model_load_path,model_resume_path,save_interval,calibrate_thresholds,randomize_init,test_with_thresholds,gfac,immediate_train,ssmize_predictor,flash_attn,train_headpredictor,min_sparse_index,attn_reduce_factor,head_attn_reduce_factor,pred_lr,dDash,skip_outlier,no_pred_causal_mask,evalgap,max_norm,intdim,token_sparse_method,eval_llm_mode,proj_name,eval_subset,train_subset_fac,rpj_train_seqlen,eval_wk2_seqlen,grad_accum_steps,producer_frequency,group_factor,num_tok_per_page,stream_llm_start_size,lfunc,no_wandb,no_wikitext_eval,result_file,wname,do_wikitext_eval,net_sparsity,true_token_sparsity,perplexity,arc_easy_acc,hellaswag_acc,piqa_acc,winogrande_acc
42,meta-llama/Llama-3.2-3B,"['winogrande', 'hellaswag', 'piqa', 'arc_easy']",False,llama,qk,128,4,c4_realnewslike,0.5,True,False,"['triviaqa', 'qasper', 'trec', 'samsum', 'lcc', 'repobench-p', 'qmsum', 'multi_news']",eval,/home/ya255/projects/all_contextual/expt_model/42_meta-llama_Llama-3.2-3B_False_llama_qk_128_4_c4_realnewslike_0.5_True_False_finetune_None_None_5000_False_False_1_False_False_False_False_4_8_2/0.001_16_None_False_1000_20_1024_fixed_40pc_ExpPred_AllContextual_Jan9_1000_4_2048_1024_1_28_4_16_4_MSE_False_False_L3_3B_2k_1PC.csv_L3_3B_2k_1PC_True_0.38571428571428584_20250111-042334.pt,,5000,False,False,False,1,False,False,False,False,4,8,2,0.001,16,,False,1000,20,1024,fixed_60pc,ExpPred,AllContextual_ICML,2500,4,1024,1024,1,28,4,16,4,MSE,True,False,L3_3B_2k_1PC.csv,L3_3B_2k_1PC,True,0.5785714285714284,56.8004588995661,9.380229949951172,0.6948653198653199,0.4616,0.7453754080522307,0.584846093133386
